{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('regression_data.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.292929</td>\n",
       "      <td>33.029204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.434343</td>\n",
       "      <td>94.117665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.939394</td>\n",
       "      <td>94.286240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.282828</td>\n",
       "      <td>30.425881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.010101</td>\n",
       "      <td>21.471686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23.464646</td>\n",
       "      <td>114.641490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.080808</td>\n",
       "      <td>50.327894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.313131</td>\n",
       "      <td>42.967290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.797980</td>\n",
       "      <td>41.549861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13.616162</td>\n",
       "      <td>70.141396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18.161616</td>\n",
       "      <td>90.402407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18.414141</td>\n",
       "      <td>79.081124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11.090909</td>\n",
       "      <td>61.964632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.757576</td>\n",
       "      <td>17.779069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>22.202020</td>\n",
       "      <td>92.061211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11.595960</td>\n",
       "      <td>81.618944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.565657</td>\n",
       "      <td>33.251843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.525253</td>\n",
       "      <td>28.360255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21.949495</td>\n",
       "      <td>91.376273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>26.242424</td>\n",
       "      <td>110.364875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.515152</td>\n",
       "      <td>15.555455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>11.399703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.040404</td>\n",
       "      <td>61.459143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18.919192</td>\n",
       "      <td>89.809026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.777778</td>\n",
       "      <td>33.061025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.646465</td>\n",
       "      <td>80.520091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26.747475</td>\n",
       "      <td>108.851132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>16.898990</td>\n",
       "      <td>102.622734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13.868687</td>\n",
       "      <td>60.228597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10.585859</td>\n",
       "      <td>62.427540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>6.545455</td>\n",
       "      <td>22.837147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>23.717172</td>\n",
       "      <td>98.510170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>7.050505</td>\n",
       "      <td>17.831074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>25.737374</td>\n",
       "      <td>115.957357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>14.121212</td>\n",
       "      <td>72.385407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>21.696970</td>\n",
       "      <td>113.959291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>17.151515</td>\n",
       "      <td>86.747102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>25.232323</td>\n",
       "      <td>133.348666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>4.272727</td>\n",
       "      <td>33.041558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>13.363636</td>\n",
       "      <td>54.612202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>7.808081</td>\n",
       "      <td>36.198957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2.252525</td>\n",
       "      <td>26.964502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>17.404040</td>\n",
       "      <td>79.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>20.181818</td>\n",
       "      <td>104.346668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>4.020202</td>\n",
       "      <td>20.852305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>18.666667</td>\n",
       "      <td>93.125419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>19.424242</td>\n",
       "      <td>90.082806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>17.909091</td>\n",
       "      <td>95.203307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>8.060606</td>\n",
       "      <td>50.256996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>10.333333</td>\n",
       "      <td>46.214974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>15.131313</td>\n",
       "      <td>56.033220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>14.878788</td>\n",
       "      <td>75.401679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>12.606061</td>\n",
       "      <td>53.703215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>12.101010</td>\n",
       "      <td>68.615176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>23.969697</td>\n",
       "      <td>105.523011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>3.262626</td>\n",
       "      <td>23.485911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>26.494949</td>\n",
       "      <td>128.020235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>8.818182</td>\n",
       "      <td>35.866577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>19.929293</td>\n",
       "      <td>96.974965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>22.707071</td>\n",
       "      <td>114.154363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0           1\n",
       "0    6.292929   33.029204\n",
       "1   20.434343   94.117665\n",
       "2   20.939394   94.286240\n",
       "3    5.282828   30.425881\n",
       "4    3.010101   21.471686\n",
       "5   23.464646  114.641490\n",
       "6   10.080808   50.327894\n",
       "7    8.313131   42.967290\n",
       "8    6.797980   41.549861\n",
       "9   13.616162   70.141396\n",
       "10  18.161616   90.402407\n",
       "11  18.414141   79.081124\n",
       "12  11.090909   61.964632\n",
       "13   2.757576   17.779069\n",
       "14  22.202020   92.061211\n",
       "15  11.595960   81.618944\n",
       "16   8.565657   33.251843\n",
       "17   4.525253   28.360255\n",
       "18  21.949495   91.376273\n",
       "19  26.242424  110.364875\n",
       "20   3.515152   15.555455\n",
       "21   2.000000   11.399703\n",
       "22   6.040404   61.459143\n",
       "23  18.919192   89.809026\n",
       "24   4.777778   33.061025\n",
       "25  16.646465   80.520091\n",
       "26  26.747475  108.851132\n",
       "27  16.898990  102.622734\n",
       "28  13.868687   60.228597\n",
       "29  10.585859   62.427540\n",
       "..        ...         ...\n",
       "70   6.545455   22.837147\n",
       "71  23.717172   98.510170\n",
       "72   7.050505   17.831074\n",
       "73  25.737374  115.957357\n",
       "74  14.121212   72.385407\n",
       "75  21.696970  113.959291\n",
       "76  17.151515   86.747102\n",
       "77  25.232323  133.348666\n",
       "78   4.272727   33.041558\n",
       "79  13.363636   54.612202\n",
       "80   7.808081   36.198957\n",
       "81   2.252525   26.964502\n",
       "82  17.404040   79.006900\n",
       "83  20.181818  104.346668\n",
       "84   4.020202   20.852305\n",
       "85  18.666667   93.125419\n",
       "86  19.424242   90.082806\n",
       "87  17.909091   95.203307\n",
       "88   8.060606   50.256996\n",
       "89  10.333333   46.214974\n",
       "90  15.131313   56.033220\n",
       "91  14.878788   75.401679\n",
       "92  12.606061   53.703215\n",
       "93  12.101010   68.615176\n",
       "94  23.969697  105.523011\n",
       "95   3.262626   23.485911\n",
       "96  26.494949  128.020235\n",
       "97   8.818182   35.866577\n",
       "98  19.929293   96.974965\n",
       "99  22.707071  114.154363\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X=df.values[:,0].reshape(-1,1),y=df.values[:,1].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 4.1117883]]), array([ 10.51]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_,np.round(reg.intercept_,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1100000000000003"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.round(4.1117883,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================================================\n",
      "Deep Belief Network features for digit classification\n",
      "==============================================================\n",
      "Adapted from http://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html#sphx-glr-auto-examples-neural-networks-plot-rbm-logistic-classification-py\n",
      "This example shows how to build a classification pipeline with a UnsupervisedDBN\n",
      "feature extractor and a :class:`LogisticRegression\n",
      "<sklearn.linear_model.LogisticRegression>` classifier. The hyperparameters\n",
      "of the entire model (learning rate, hidden layer size, regularization)\n",
      "were optimized by grid search, but the search is not reproduced here because\n",
      "of runtime constraints.\n",
      "Logistic regression on raw pixel values is presented for comparison. The\n",
      "example shows that the features extracted by the UnsupervisedDBN help improve the\n",
      "classification accuracy.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalidus/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 1.036762\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.786044\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.696757\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.678937\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.617023\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.597065\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.545181\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.583840\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.557136\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.516251\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.531208\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.547970\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.528397\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.505231\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.510072\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.504495\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.500497\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.498268\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.482406\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.490693\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 1.316233\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 0.970626\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 0.806698\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 0.718767\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 0.604589\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 0.633443\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 0.603380\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 0.594177\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 0.519945\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 0.527960\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 0.537891\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 0.507582\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 0.488586\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 0.505693\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 0.498633\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 0.445051\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 0.495827\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 0.468829\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 0.492530\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 0.476339\n",
      "[END] Pre-training step\n",
      "\n",
      "Logistic regression using RBM features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99       174\n",
      "          1       0.95      0.98      0.97       184\n",
      "          2       0.98      0.99      0.99       166\n",
      "          3       0.99      0.97      0.98       194\n",
      "          4       0.98      0.98      0.98       186\n",
      "          5       0.98      0.97      0.97       181\n",
      "          6       1.00      0.98      0.99       207\n",
      "          7       0.99      1.00      0.99       154\n",
      "          8       0.97      0.95      0.96       182\n",
      "          9       0.96      0.96      0.96       169\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1797\n",
      "\n",
      "\n",
      "Logistic regression using raw pixel features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.94      0.89       174\n",
      "          1       0.57      0.55      0.56       184\n",
      "          2       0.72      0.85      0.78       166\n",
      "          3       0.76      0.74      0.75       194\n",
      "          4       0.85      0.82      0.84       186\n",
      "          5       0.74      0.75      0.75       181\n",
      "          6       0.93      0.88      0.91       207\n",
      "          7       0.86      0.90      0.88       154\n",
      "          8       0.68      0.55      0.61       182\n",
      "          9       0.71      0.74      0.72       169\n",
      "\n",
      "avg / total       0.77      0.77      0.77      1797\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "==============================================================\n",
    "Deep Belief Network features for digit classification\n",
    "==============================================================\n",
    "Adapted from http://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html#sphx-glr-auto-examples-neural-networks-plot-rbm-logistic-classification-py\n",
    "This example shows how to build a classification pipeline with a UnsupervisedDBN\n",
    "feature extractor and a :class:`LogisticRegression\n",
    "<sklearn.linear_model.LogisticRegression>` classifier. The hyperparameters\n",
    "of the entire model (learning rate, hidden layer size, regularization)\n",
    "were optimized by grid search, but the search is not reproduced here because\n",
    "of runtime constraints.\n",
    "Logistic regression on raw pixel values is presented for comparison. The\n",
    "example shows that the features extracted by the UnsupervisedDBN help improve the\n",
    "classification accuracy.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.ndimage import convolve\n",
    "from sklearn import linear_model, datasets, metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from dbn.models import UnsupervisedDBN # use \"from dbn.tensorflow import SupervisedDBNClassification\" for computations on TensorFlow\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Setting up\n",
    "\n",
    "def nudge_dataset(X, Y):\n",
    "    \"\"\"\n",
    "    This produces a dataset 5 times bigger than the original one,\n",
    "    by moving the 8x8 images in X around by 1px to left, right, down, up\n",
    "    \"\"\"\n",
    "    direction_vectors = [\n",
    "        [[0, 1, 0],\n",
    "         [0, 0, 0],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [1, 0, 0],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [0, 0, 1],\n",
    "         [0, 0, 0]],\n",
    "\n",
    "        [[0, 0, 0],\n",
    "         [0, 0, 0],\n",
    "         [0, 1, 0]]]\n",
    "\n",
    "    shift = lambda x, w: convolve(x.reshape((8, 8)), mode='constant',\n",
    "                                  weights=w).ravel()\n",
    "    X = np.concatenate([X] +\n",
    "                       [np.apply_along_axis(shift, 1, X, vector)\n",
    "                        for vector in direction_vectors])\n",
    "    Y = np.concatenate([Y for _ in range(5)], axis=0)\n",
    "    return X, Y\n",
    "\n",
    "# Load Data\n",
    "digits = datasets.load_digits()\n",
    "X = np.asarray(digits.data, 'float32')\n",
    "X, Y = nudge_dataset(X, digits.target)\n",
    "X = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)  # 0-1 scaling\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "# Models we will use\n",
    "logistic = linear_model.LogisticRegression()\n",
    "dbn = UnsupervisedDBN(hidden_layers_structure=[256, 512],\n",
    "                      batch_size=10,\n",
    "                      learning_rate_rbm=0.06,\n",
    "                      n_epochs_rbm=20,\n",
    "                      activation_function='sigmoid')\n",
    "\n",
    "classifier = Pipeline(steps=[('dbn', dbn),\n",
    "                             ('logistic', logistic)])\n",
    "\n",
    "###############################################################################\n",
    "# Training\n",
    "logistic.C = 6000.0\n",
    "\n",
    "# Training RBM-Logistic Pipeline\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "# Training Logistic regression\n",
    "logistic_classifier = linear_model.LogisticRegression(C=100.0)\n",
    "logistic_classifier.fit(X_train, Y_train)\n",
    "\n",
    "###############################################################################\n",
    "# Evaluation\n",
    "\n",
    "print()\n",
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(\n",
    "        Y_test,\n",
    "        classifier.predict(X_test))))\n",
    "\n",
    "print(\"Logistic regression using raw pixel features:\\n%s\\n\" % (\n",
    "    metrics.classification_report(\n",
    "        Y_test,\n",
    "        logistic_classifier.predict(X_test))))\n",
    "\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('regression_data(1).csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'dbn.tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8adc46a53861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSupervisedDBNClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# use \"from dbn import SupervisedDBNClassification\" for computations on CPU with numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'dbn.tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.classification import accuracy_score\n",
    "\n",
    "from dbn. import SupervisedDBNClassification\n",
    "# use \"from dbn import SupervisedDBNClassification\" for computations on CPU with numpy\n",
    "\n",
    "# Loading dataset\n",
    "digits = load_digits()\n",
    "X, Y = digits.data, digits.target\n",
    "\n",
    "# Data scaling\n",
    "X = (X / 16).astype(np.float32)\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Training\n",
    "classifier = SupervisedDBNClassification(hidden_layers_structure=[256, 256],\n",
    "                                         learning_rate_rbm=0.05,\n",
    "                                         learning_rate=0.1,\n",
    "                                         n_epochs_rbm=10,\n",
    "                                         n_iter_backprop=100,\n",
    "                                         batch_size=32,\n",
    "                                         activation_function='relu',\n",
    "                                         dropout_p=0.2)\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "# Save the model\n",
    "classifier.save('model.pkl')\n",
    "\n",
    "# Restore it\n",
    "classifier = SupervisedDBNClassification.load('model.pkl')\n",
    "\n",
    "# Test\n",
    "Y_pred = classifier.predict(X_test)\n",
    "print('Done.\\nAccuracy: %f' % accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
